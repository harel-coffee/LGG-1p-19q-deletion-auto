{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import interp, stats\n",
    "from imblearn.over_sampling import ADASYN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILES (REGARDING FREQUENCY OF FEATURES)\n",
    "all_features_list_df_cub = pd.read_csv(\"training_cubic_all_features_list_result.csv\",index_col=False)\n",
    "all_features_count_df_cub = all_features_list_df_cub.stack().value_counts() # it returns a df with the frequency for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILE AND CREATE A PD DATAFRAME WITH ONLY THE FIRST n SELECTED FEATURES - TRAINING DATASET\n",
    "\n",
    "first_n_features_to_select_cub = 7 # choose the value\n",
    "\n",
    "# load the original dataset\n",
    "training_dataframe_df_cub = pd.read_csv(\"training - cubic after WEKA CfsSubsetEval.csv\",index_col='exam')\n",
    "size_mapping = {\"codeletion\":0,\"noncodeletion\":1} # MAPPING for outcome \n",
    "training_dataframe_df_cub[\"outcome\"] = training_dataframe_df_cub[\"outcome\"].map(size_mapping)\n",
    "\n",
    "training_feature_names_cub = [x[2:-2] for x in [*all_features_count_df_cub.index]]\n",
    "training_selected_features_cub = training_feature_names_cub[:first_n_features_to_select_cub]\n",
    "training_New_dataframe_cub = training_dataframe_df_cub[training_selected_features_cub]\n",
    "training_New_dataframe_cub[\"outcome\"] = training_dataframe_df_cub[\"outcome\"] \n",
    "training_dataframe_with_selected_features_df_cub = training_New_dataframe_cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILE AND CREATE A PD DATAFRAME WITH ONLY THE FIRST n SELECTED FEATURES - TESTING DATASET\n",
    "\n",
    "first_n_features_to_select_cub = 7 # choose the value\n",
    "\n",
    "# load the original dataset\n",
    "testing_dataframe_df_cub = pd.read_csv(\"testing - cubic.csv\",index_col='exam', encoding = \"ISO-8859-1\") # insert the all original dataset\n",
    "size_mapping = {\"codeletion\":0,\"noncodeletion\":1} # MAPPING for outcome \n",
    "testing_dataframe_df_cub[\"outcome\"] = testing_dataframe_df_cub[\"outcome\"].map(size_mapping)\n",
    "\n",
    "testing_feature_names_cub = [x[3:-3] for x in [*all_features_count_df_cub.index]]\n",
    "testing_selected_features_cub = testing_feature_names_cub[:first_n_features_to_select_cub]\n",
    "testing_New_dataframe_cub = testing_dataframe_df_cub[testing_selected_features_cub]\n",
    "testing_New_dataframe_cub[\"outcome\"] = testing_dataframe_df_cub[\"outcome\"]\n",
    "testing_dataframe_with_selected_features_df_cub = testing_New_dataframe_cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The chosen features are:\", [x[1:-1] for x in [*training_selected_features_cub]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the training dataset and testing the model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cub = RandomForestClassifier(random_state=1, n_estimators=100) # Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename dataframes into X_train_cub, Y_train_cub, X_test_cub, Y_test_cub (numpy arrays)\n",
    "\n",
    "Y_train_cub = training_dataframe_with_selected_features_df_cub['outcome']\n",
    "X_train_cub = training_dataframe_with_selected_features_df_cub.drop('outcome',axis=1)\n",
    "\n",
    "Y_test_cub = testing_dataframe_with_selected_features_df_cub['outcome']\n",
    "X_test_cub = testing_dataframe_with_selected_features_df_cub.drop('outcome',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "ss = StandardScaler() \n",
    "X_train_SS_np_cub = ss.fit_transform(X_train_cub) \n",
    "X_train_SS_cub = pd.DataFrame(X_train_SS_np_cub, index=X_train_cub.index, columns=X_train_cub.columns)\n",
    "X_test_SS_np_cub = ss.transform(X_test_cub) \n",
    "X_test_SS_cub = pd.DataFrame(X_test_SS_np_cub, index=X_test_cub.index, columns=X_test_cub.columns)\n",
    "\n",
    "# ADASYN\n",
    "sm = ADASYN(random_state=1)\n",
    "X_train_SS_balanced_np_cub, Y_train_balanced_np_cub = sm.fit_sample(X_train_SS_cub, Y_train_cub)\n",
    "X_train_SS_balanced_cub = pd.DataFrame(X_train_SS_balanced_np_cub, columns=X_train_SS_cub.columns)\n",
    "Y_train_balanced_cub = pd.DataFrame(Y_train_balanced_np_cub, columns=[\"outcome\"])\n",
    "\n",
    "# Fitting the model\n",
    "model_cub.fit (X_train_SS_balanced_cub, Y_train_balanced_cub)\n",
    "\n",
    "# Compute predictions, probabilities and accuracy\n",
    "predictions_cub = model_cub.predict(X_test_SS_cub)\n",
    "probabilities_cub = model_cub.predict_proba(X_test_SS_cub)\n",
    "accuracy_cub = accuracy_score(Y_test_cub, predictions_cub)\n",
    "\n",
    "# Compute AUC\n",
    "fpr_cub, tpr_cub, threshold_cub = roc_curve(Y_test_cub, np.array(probabilities_cub)[:,1])\n",
    "roc_auc_cub = auc(fpr_cub, tpr_cub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the values for bootstrap code and De-Long test\n",
    "y_true_cub = np.array(Y_test_cub)\n",
    "y_pred_cub = np.array(predictions_cub)\n",
    "y_prob_cub = np.array(probabilities_cub)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Confusion Matrix\n",
    "print (\"Confusion matrix for cubic features: \\n\", confusion_matrix(y_true_cub, y_pred_cub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform BOOTSTRAP with y_true, predictions, probabilities\n",
    "\n",
    "n_bootstraps = 10000\n",
    "rng_seed = 1  # control reproducibility\n",
    "\n",
    "bootstrapped_acc_cub = []\n",
    "bootstrapped_auc_cub = []\n",
    "bootstrapped_sens_cub = []\n",
    "bootstrapped_spec_cub = []\n",
    "\n",
    "bootstrapped_tpr_cub = []\n",
    "bootstrapped_fpr_cub = []\n",
    "bootstrapped_thr_cub = []\n",
    "\n",
    "bootstrapped_tprs_cub = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices_0=np.where(y_true_cub == 0)\n",
    "    indices_1=np.where(y_true_cub == 1)\n",
    "    \n",
    "    # 'balanced bootstrapping'\n",
    "    random_indices_0=rng.choice(indices_0[0],len(indices_0[0]))\n",
    "    random_indices_1=rng.choice(indices_1[0],len(indices_0[0]))\n",
    "    random_indices=np.concatenate((random_indices_0,random_indices_1), axis=None)\n",
    "\n",
    "    acc_cub = accuracy_score(y_true_cub[random_indices], y_pred_cub[random_indices])\n",
    "    auc_cub = roc_auc_score(y_true_cub[random_indices], y_prob_cub[random_indices])\n",
    "    sens_cub = recall_score(y_true_cub[random_indices], y_pred_cub[random_indices], pos_label=1)\n",
    "    spec_cub = recall_score(y_true_cub[random_indices], y_pred_cub[random_indices], pos_label=0)\n",
    "    \n",
    "    fpr_cub, tpr_cub, threshold_cub = roc_curve(y_true_cub[random_indices], y_prob_cub[random_indices])\n",
    "    \n",
    "    interp_tpr_cub = interp(mean_fpr, fpr_cub, tpr_cub)\n",
    "    interp_tpr_cub[0] = 0.0\n",
    "    bootstrapped_tprs_cub.append(interp_tpr_cub)\n",
    "    \n",
    "    bootstrapped_acc_cub.append(acc_cub)\n",
    "    bootstrapped_auc_cub.append(auc_cub)\n",
    "    bootstrapped_sens_cub.append(sens_cub)\n",
    "    bootstrapped_spec_cub.append(spec_cub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics distributions for bootstrapping steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(bootstrapped_acc_cub)\n",
    "plt.title('Acc cub')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(bootstrapped_auc_cub)\n",
    "plt.title('AUC cub')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(bootstrapped_sens_cub)\n",
    "plt.title('Sens cub')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(bootstrapped_spec_cub)\n",
    "plt.title('Spec cub')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distr normality test (Shapiro-Wilcoxon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acc cub: ', stats.shapiro(bootstrapped_acc_cub))\n",
    "print ('AUC cub: ', stats.shapiro(bootstrapped_auc_cub))\n",
    "print ('Sens cub: ', stats.shapiro(bootstrapped_sens_cub))\n",
    "print ('Spec cub: ', stats.shapiro(bootstrapped_spec_cub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values are small -> distr is not normal -> estimation should be represented as median (low_percentile, up_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acc cub: {} ({}, {})'.format(np.median(bootstrapped_acc_cub), np.percentile(bootstrapped_acc_cub, 2.5), np.percentile(bootstrapped_acc_cub, 97.5)))\n",
    "print ('AUC cub: {} ({}, {})'.format(np.median(bootstrapped_auc_cub), np.percentile(bootstrapped_auc_cub, 2.5), np.percentile(bootstrapped_auc_cub, 97.5)))\n",
    "print ('Sens cub: {} ({}, {})'.format(np.median(bootstrapped_sens_cub), np.percentile(bootstrapped_sens_cub, 2.5), np.percentile(bootstrapped_sens_cub, 97.5)))\n",
    "print ('Spec cub: {} ({}, {})'.format(np.median(bootstrapped_spec_cub), np.percentile(bootstrapped_spec_cub, 2.5), np.percentile(bootstrapped_spec_cub, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC CURVE AND AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "\n",
    "plt.title('ROC Validation dataset')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "mean_tpr_cub = np.median(bootstrapped_tprs_cub, axis=0)  \n",
    "mean_tpr_cub[-1] = 1.0                \n",
    "plt.plot(mean_fpr, mean_tpr_cub, color='b', \n",
    "        label=r'Median ROC (AUC = %0.2f)' % (np.median(bootstrapped_auc_cub)), \n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "tprs_upper = np.percentile(bootstrapped_tprs_cub, 2.5,  axis = 0)\n",
    "tprs_lower = np.percentile(bootstrapped_tprs_cub, 97.5, axis = 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95 % CI')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
