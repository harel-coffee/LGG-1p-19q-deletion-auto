{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import interp, stats\n",
    "from imblearn.over_sampling import ADASYN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILES (REGARDING FREQUENCY OF FEATURES)\n",
    "all_features_list_df_lin = pd.read_csv(\"training_linear_all_features_list_result.csv\",index_col=False)\n",
    "all_features_count_df_lin = all_features_list_df_lin.stack().value_counts() # it returns a df with the frequency for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILE AND CREATE A PD DATAFRAME WITH ONLY THE FIRST n SELECTED FEATURES - TRAINING DATASET\n",
    "\n",
    "first_n_features_to_select_lin = 5 # choose the value\n",
    "\n",
    "# load the original dataset\n",
    "training_dataframe_df_lin = pd.read_csv(\"training - linear after WEKA CfsSubsetEval.csv\",index_col='exam')\n",
    "size_mapping = {\"codeletion\":0,\"noncodeletion\":1} # MAPPING for outcome \n",
    "training_dataframe_df_lin[\"outcome\"] = training_dataframe_df_lin[\"outcome\"].map(size_mapping)\n",
    "\n",
    "training_feature_names_lin = [x[2:-2] for x in [*all_features_count_df_lin.index]]\n",
    "training_selected_features_lin = training_feature_names_lin[:first_n_features_to_select_lin]\n",
    "training_New_dataframe_lin = training_dataframe_df_lin[training_selected_features_lin]\n",
    "training_New_dataframe_lin[\"outcome\"] = training_dataframe_df_lin[\"outcome\"] \n",
    "training_dataframe_with_selected_features_df_lin = training_New_dataframe_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILE AND CREATE A PD DATAFRAME WITH ONLY THE FIRST n SELECTED FEATURES - TESTING DATASET\n",
    "\n",
    "first_n_features_to_select_lin = 5 # choose the value\n",
    "\n",
    "# load the original dataset\n",
    "testing_dataframe_df_lin = pd.read_csv(\"testing - linear.csv\",index_col='exam', encoding = \"ISO-8859-1\") # insert the all original dataset\n",
    "size_mapping = {\"codeletion\":0,\"noncodeletion\":1} # MAPPING for outcome \n",
    "testing_dataframe_df_lin[\"outcome\"] = testing_dataframe_df_lin[\"outcome\"].map(size_mapping)\n",
    "\n",
    "testing_feature_names_lin = [x[3:-3] for x in [*all_features_count_df_lin.index]]\n",
    "testing_selected_features_lin = testing_feature_names_lin[:first_n_features_to_select_lin]\n",
    "testing_New_dataframe_lin = testing_dataframe_df_lin[testing_selected_features_lin]\n",
    "testing_New_dataframe_lin[\"outcome\"] = testing_dataframe_df_lin[\"outcome\"]\n",
    "testing_dataframe_with_selected_features_df_lin = testing_New_dataframe_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The chosen features are:\", [x[1:-1] for x in [*training_selected_features_lin]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the training dataset and testing the model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = RandomForestClassifier(random_state=1, n_estimators=100) # Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename dataframes into X_train_lin, Y_train_lin, X_test_lin, Y_test_lin (numpy arrays)\n",
    "\n",
    "Y_train_lin = training_dataframe_with_selected_features_df_lin['outcome']\n",
    "X_train_lin = training_dataframe_with_selected_features_df_lin.drop('outcome',axis=1)\n",
    "\n",
    "Y_test_lin = testing_dataframe_with_selected_features_df_lin['outcome']\n",
    "X_test_lin = testing_dataframe_with_selected_features_df_lin.drop('outcome',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "ss = StandardScaler() \n",
    "X_train_SS_np_lin = ss.fit_transform(X_train_lin) \n",
    "X_train_SS_lin=pd.DataFrame(X_train_SS_np_lin, index=X_train_lin.index, columns=X_train_lin.columns)\n",
    "X_test_SS_np_lin = ss.transform(X_test_lin) \n",
    "X_test_SS_lin=pd.DataFrame(X_test_SS_np_lin, index=X_test_lin.index, columns=X_test_lin.columns)\n",
    "\n",
    "# ADASYN\n",
    "sm = ADASYN(random_state=1)\n",
    "X_train_SS_balanced_np_lin, Y_train_balanced_np_lin = sm.fit_sample(X_train_SS_lin, Y_train_lin)\n",
    "X_train_SS_balanced_lin=pd.DataFrame(X_train_SS_balanced_np_lin, columns=X_train_SS_lin.columns)\n",
    "Y_train_balanced_lin=pd.DataFrame(Y_train_balanced_np_lin, columns=[\"outcome\"])\n",
    "\n",
    "# Fitting the model\n",
    "model_lin.fit (X_train_SS_balanced_lin, Y_train_balanced_lin)\n",
    "\n",
    "# Compute predictions, probabilities and accuracy\n",
    "predictions_lin = model_lin.predict(X_test_SS_lin)\n",
    "probabilities_lin = model_lin.predict_proba(X_test_SS_lin)\n",
    "accuracy_lin = accuracy_score(Y_test_lin, predictions_lin)\n",
    "\n",
    "# Compute AUC\n",
    "fpr_lin, tpr_lin, threshold_lin = roc_curve(Y_test_lin, np.array(probabilities_lin)[:,1])\n",
    "roc_auc_lin = auc(fpr_lin, tpr_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the values for bootstrap code and De-Long test\n",
    "y_true_lin = np.array(Y_test_lin)\n",
    "y_pred_lin = np.array(predictions_lin)\n",
    "y_prob_lin = np.array(probabilities_lin)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print Confusion Matrix\n",
    "print (\"Confusion matrix for linear features: \\n\", confusion_matrix(y_true_lin, y_pred_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform BOOTSTRAP with y_true, predictions, probabilities\n",
    "\n",
    "n_bootstraps = 10000\n",
    "rng_seed = 1  # control reproducibility\n",
    "\n",
    "bootstrapped_acc_lin = []\n",
    "bootstrapped_auc_lin = []\n",
    "bootstrapped_sens_lin = []\n",
    "bootstrapped_spec_lin = []\n",
    "\n",
    "bootstrapped_tpr_lin = []\n",
    "bootstrapped_fpr_lin = []\n",
    "bootstrapped_thr_lin = []\n",
    "\n",
    "bootstrapped_tprs_lin = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices_0=np.where(y_true_lin == 0)\n",
    "    indices_1=np.where(y_true_lin == 1)\n",
    "    \n",
    "    # 'balanced bootstrapping'\n",
    "    random_indices_0=rng.choice(indices_0[0],len(indices_0[0]))\n",
    "    random_indices_1=rng.choice(indices_1[0],len(indices_0[0]))\n",
    "    random_indices=np.concatenate((random_indices_0,random_indices_1), axis=None)\n",
    "\n",
    "    acc_lin = accuracy_score(y_true_lin[random_indices], y_pred_lin[random_indices])\n",
    "    auc_lin = roc_auc_score(y_true_lin[random_indices], y_prob_lin[random_indices])\n",
    "    sens_lin = recall_score(y_true_lin[random_indices], y_pred_lin[random_indices], pos_label=1)\n",
    "    spec_lin = recall_score(y_true_lin[random_indices], y_pred_lin[random_indices], pos_label=0)\n",
    "    \n",
    "    fpr_lin, tpr_lin, threshold_lin = roc_curve(y_true_lin[random_indices], y_prob_lin[random_indices])\n",
    "    \n",
    "    interp_tpr_lin = interp(mean_fpr, fpr_lin, tpr_lin)\n",
    "    interp_tpr_lin[0] = 0.0\n",
    "    bootstrapped_tprs_lin.append(interp_tpr_lin)\n",
    "    \n",
    "    bootstrapped_acc_lin.append(acc_lin)\n",
    "    bootstrapped_auc_lin.append(auc_lin)\n",
    "    bootstrapped_sens_lin.append(sens_lin)\n",
    "    bootstrapped_spec_lin.append(spec_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics distributions for bootstrapping steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(bootstrapped_acc_lin)\n",
    "plt.title('Acc lin')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(bootstrapped_auc_lin)\n",
    "plt.title('AUC lin')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(bootstrapped_sens_lin)\n",
    "plt.title('Sens lin')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(bootstrapped_spec_lin)\n",
    "plt.title('Spec lin')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distr normality test (Shapiro-Wilcoxon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acc lin: ', stats.shapiro(bootstrapped_acc_lin))\n",
    "print ('AUC lin: ', stats.shapiro(bootstrapped_auc_lin))\n",
    "print ('Sens lin: ', stats.shapiro(bootstrapped_sens_lin))\n",
    "print ('Spec lin: ', stats.shapiro(bootstrapped_spec_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values are small -> distr is not normal -> estimation should be represented as median (low_percentile, up_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acc lin: {} ({}, {})'.format(np.median(bootstrapped_acc_lin), np.percentile(bootstrapped_acc_lin, 2.5), np.percentile(bootstrapped_acc_lin, 97.5)))\n",
    "print ('AUC lin: {} ({}, {})'.format(np.median(bootstrapped_auc_lin), np.percentile(bootstrapped_auc_lin, 2.5), np.percentile(bootstrapped_auc_lin, 97.5)))\n",
    "print ('Sens lin: {} ({}, {})'.format(np.median(bootstrapped_sens_lin), np.percentile(bootstrapped_sens_lin, 2.5), np.percentile(bootstrapped_sens_lin, 97.5)))\n",
    "print ('Spec lin: {} ({}, {})'.format(np.median(bootstrapped_spec_lin), np.percentile(bootstrapped_spec_lin, 2.5), np.percentile(bootstrapped_spec_lin, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC CURVE AND AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "\n",
    "plt.title('ROC Validation dataset')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "mean_tpr_lin = np.median(bootstrapped_tprs_lin, axis=0)  \n",
    "mean_tpr_lin[-1] = 1.0                \n",
    "plt.plot(mean_fpr, mean_tpr_lin, color='b', \n",
    "        label=r'Median ROC (AUC = %0.2f)' % (np.median(bootstrapped_auc_lin)), \n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "tprs_upper = np.percentile(bootstrapped_tprs_lin, 2.5,  axis = 0)\n",
    "tprs_lower = np.percentile(bootstrapped_tprs_lin, 97.5, axis = 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95 % CI')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
