{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import interp, stats\n",
    "from imblearn.over_sampling import ADASYN \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILES (REGARDING FREQUENCY OF FEATURES)\n",
    "all_features_list_df_lin=pd.read_csv(\"training_linear_all_features_list_result.csv\",index_col=False)\n",
    "all_features_count_df_lin=all_features_list_df_lin.stack().value_counts() # it returns a df with the frequency for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPORT CVS FILE AND CREATE A PD DATAFRAME WITH ONLY THE FIRST n SELECTED FEATURES - TRAINING DATASET\n",
    "\n",
    "first_n_features_to_select_lin=5 # choose the value\n",
    "\n",
    "# load the original dataset\n",
    "training_dataframe_df_lin=pd.read_csv(\"training - linear after WEKA CfsSubsetEval.csv\",index_col='exam')\n",
    "size_mapping={\"codeletion\":0,\"noncodeletion\":1} # MAPPING for outcome \n",
    "training_dataframe_df_lin[\"outcome\"]=training_dataframe_df_lin[\"outcome\"].map(size_mapping)\n",
    "\n",
    "training_feature_names_lin = [x[2:-2] for x in [*all_features_count_df_lin.index]]\n",
    "training_selected_features_lin = training_feature_names_lin[:first_n_features_to_select_lin]\n",
    "training_New_dataframe_lin = training_dataframe_df_lin[training_selected_features_lin]\n",
    "training_New_dataframe_lin[\"outcome\"] = training_dataframe_df_lin[\"outcome\"] \n",
    "training_dataframe_with_selected_features_df_lin = training_New_dataframe_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The chosen features are:\", [x[1:-1] for x in [*training_selected_features_lin]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the model on training dataset (CROSS VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = RandomForestClassifier(random_state=1, n_estimators=100) # Choose the model\n",
    "CV = 10 # Choose the number of folds for cross validation\n",
    "cv = StratifiedKFold(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename dataframe into X_np, Y_np (numpy arrays)\n",
    "X_np_lin=training_dataframe_with_selected_features_df_lin.drop('outcome',axis=1).values\n",
    "Y_np_lin=training_dataframe_with_selected_features_df_lin['outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation\n",
    "\n",
    "Y_trues_lin = []\n",
    "Y_predictions_lin = []\n",
    "Y_probabilities_lin = []\n",
    "all_accuracies_lin = []\n",
    "aucs_lin = []\n",
    "mean_fpr_lin = np.linspace(0, 1, 100)\n",
    "\n",
    "for train_lin, test_lin in cv.split(X_np_lin, Y_np_lin):\n",
    "    \n",
    "    # StandardScaler\n",
    "    ss = StandardScaler() \n",
    "    X_train_CV_SS_np_lin = ss.fit_transform(X_np_lin[train_lin]) \n",
    "    X_test_CV_SS_np_lin = ss.transform(X_np_lin[test_lin])\n",
    "    \n",
    "    # ADASYN\n",
    "    sm = ADASYN(random_state=1)    \n",
    "    X_train_CV_SS_BAL_np_lin, Y_train_CV_balanced_lin = sm.fit_sample(X_train_CV_SS_np_lin, Y_np_lin[train_lin])\n",
    "    \n",
    "    X_for_CV_model_training_lin = X_train_CV_SS_BAL_np_lin  \n",
    "    Y_for_CV_model_training_lin = Y_train_CV_balanced_lin\n",
    "    \n",
    "    # fitting the model\n",
    "    model_lin.fit (X_for_CV_model_training_lin, Y_for_CV_model_training_lin)\n",
    "    \n",
    "    # Compute prediction, probabilities and accuracy\n",
    "    pred_lin_ = model_lin.predict(X_test_CV_SS_np_lin)\n",
    "    probas_lin_ = model_lin.predict_proba(X_test_CV_SS_np_lin)\n",
    "    accuracy_lin_ = accuracy_score(Y_np_lin[test_lin], pred_lin_)\n",
    "    \n",
    "    # Compute AUC\n",
    "    fpr_lin, tpr_lin, thresholds_lin = roc_curve(Y_np_lin[test_lin], probas_lin_[:, 1])   \n",
    "    roc_auc_lin = auc(fpr_lin, tpr_lin)    \n",
    "    \n",
    "    # Store data\n",
    "    aucs_lin.append(roc_auc_lin)\n",
    "    Y_trues_lin.extend(Y_np_lin[test_lin])\n",
    "    Y_predictions_lin.extend(pred_lin_)\n",
    "    Y_probabilities_lin.extend(probas_lin_)\n",
    "    all_accuracies_lin.append(accuracy_lin_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the values for bootstrap code and De-Long test\n",
    "y_true_lin = np.array(Y_trues_lin)\n",
    "y_pred_lin = np.array(Y_predictions_lin)\n",
    "y_prob_lin = np.array(Y_probabilities_lin)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Confusion Matrix\n",
    "print (\"Confusion matrix for linear features (Cross Validation - training dataset): \\n\", confusion_matrix(y_true_lin, y_pred_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bootstrap with y_true, predictions, probabilities from CV model\n",
    "\n",
    "n_bootstraps = 10000\n",
    "rng_seed = 1  # control reproducibility\n",
    "\n",
    "bootstrapped_acc_lin = []\n",
    "bootstrapped_auc_lin = []\n",
    "bootstrapped_sens_lin = []\n",
    "bootstrapped_spec_lin = []\n",
    "\n",
    "bootstrapped_tpr_lin = []\n",
    "bootstrapped_fpr_lin = []\n",
    "bootstrapped_thr_lin = []\n",
    "\n",
    "bootstrapped_tprs_lin = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices_0=np.where(y_true_lin == 0)\n",
    "    indices_1=np.where(y_true_lin == 1)\n",
    "    \n",
    "    # 'balanced bootstrapping'\n",
    "    random_indices_0=rng.choice(indices_0[0],len(indices_0[0]))\n",
    "    random_indices_1=rng.choice(indices_1[0],len(indices_0[0]))\n",
    "    random_indices=np.concatenate((random_indices_0,random_indices_1), axis=None)\n",
    "    \n",
    "    acc_lin = accuracy_score(y_true_lin[random_indices], y_pred_lin[random_indices])\n",
    "    auc_lin = roc_auc_score(y_true_lin[random_indices], y_prob_lin[random_indices])\n",
    "    sens_lin = recall_score(y_true_lin[random_indices], y_pred_lin[random_indices], pos_label=1)\n",
    "    spec_lin = recall_score(y_true_lin[random_indices], y_pred_lin[random_indices], pos_label=0)\n",
    "    \n",
    "    fpr_lin, tpr_lin, threshold_lin = roc_curve(y_true_lin[random_indices], y_prob_lin[random_indices])\n",
    "    \n",
    "    interp_tpr_lin = interp(mean_fpr, fpr_lin, tpr_lin)\n",
    "    interp_tpr_lin[0] = 0.0\n",
    "    bootstrapped_tprs_lin.append(interp_tpr_lin)\n",
    "    \n",
    "    bootstrapped_acc_lin.append(acc_lin)\n",
    "    bootstrapped_auc_lin.append(auc_lin)\n",
    "    bootstrapped_sens_lin.append(sens_lin)\n",
    "    bootstrapped_spec_lin.append(spec_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics distributions for bootstrapping steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(bootstrapped_acc_lin)\n",
    "plt.title('Acc lin')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(bootstrapped_auc_lin)\n",
    "plt.title('AUC lin')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(bootstrapped_sens_lin)\n",
    "plt.title('Sens lin')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(bootstrapped_spec_lin)\n",
    "plt.title('Spec lin')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distr normality test (Shapiro-Wilcoxon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Acc lin: ', stats.shapiro(bootstrapped_acc_lin))\n",
    "print ('AUC lin: ', stats.shapiro(bootstrapped_auc_lin))\n",
    "print ('Sens lin: ', stats.shapiro(bootstrapped_sens_lin))\n",
    "print ('Spec lin: ', stats.shapiro(bootstrapped_spec_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values are small -> distr is not normal -> estimation should be represented as median (low_percentile, up_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acc lin: {} ({}, {})'.format(np.median(bootstrapped_acc_lin), np.percentile(bootstrapped_acc_lin, 2.5), np.percentile(bootstrapped_acc_lin, 97.5)))\n",
    "print ('AUC lin: {} ({}, {})'.format(np.median(bootstrapped_auc_lin), np.percentile(bootstrapped_auc_lin, 2.5), np.percentile(bootstrapped_auc_lin, 97.5)))\n",
    "print ('Sens lin: {} ({}, {})'.format(np.median(bootstrapped_sens_lin), np.percentile(bootstrapped_sens_lin, 2.5), np.percentile(bootstrapped_sens_lin, 97.5)))\n",
    "print ('Spec lin: {} ({}, {})'.format(np.median(bootstrapped_spec_lin), np.percentile(bootstrapped_spec_lin, 2.5), np.percentile(bootstrapped_spec_lin, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC CURVE AND AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "\n",
    "plt.title('ROC Validation dataset')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "mean_tpr_lin = np.median(bootstrapped_tprs_lin, axis=0)  \n",
    "mean_tpr_lin[-1] = 1.0                \n",
    "plt.plot(mean_fpr, mean_tpr_lin, color='b', \n",
    "        label=r'Median ROC (AUC = %0.2f)' % (np.median(bootstrapped_auc_lin)), \n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "tprs_upper = np.percentile(bootstrapped_tprs_lin, 2.5,  axis = 0)\n",
    "tprs_lower = np.percentile(bootstrapped_tprs_lin, 97.5, axis = 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='95 % CI')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
